use qc_01_peer_discovery::{
    IpAddr, KademliaConfig, NodeId, PeerInfo, RoutingTable, SocketAddr, Timestamp,
};
use std::sync::atomic::{AtomicUsize, Ordering};
use std::sync::Arc;
use std::time::Instant;
use tokio::sync::Mutex;

/// BRUTAL TEST: 100 concurrent peer floods
#[tokio::test(flavor = "multi_thread", worker_threads = 16)]
async fn brutal_peer_discovery_100_flood() {
    let local_id = NodeId::new([0u8; 32]);
    let config = KademliaConfig::default();
    let table = Arc::new(Mutex::new(RoutingTable::new(local_id, config)));

    let staged = Arc::new(AtomicUsize::new(0));
    let verified = Arc::new(AtomicUsize::new(0));
    let rejected = Arc::new(AtomicUsize::new(0));

    let start = Instant::now();
    let mut handles = vec![];

    for attacker in 0..100u8 {
        let table_clone = table.clone();
        let staged_count = staged.clone();
        let verified_count = verified.clone();
        let rejected_count = rejected.clone();

        handles.push(tokio::spawn(async move {
            for i in 0..50u8 {
                let mut id_bytes = [0u8; 32];
                id_bytes[0] = attacker;
                id_bytes[1] = i;
                id_bytes[31] = attacker ^ i; // Vary distance

                let node_id = NodeId::new(id_bytes);
                let peer = PeerInfo::new(
                    node_id,
                    SocketAddr::new(IpAddr::v4(10, attacker, i, 1), 30303),
                    Timestamp::new(1000),
                );

                let mut guard = table_clone.lock().await;

                if guard.stage_peer(peer, Timestamp::new(1000)).is_ok() {
                    staged_count.fetch_add(1, Ordering::Relaxed);

                    if guard
                        .on_verification_result(&node_id, true, Timestamp::new(1001))
                        .is_ok()
                    {
                        verified_count.fetch_add(1, Ordering::Relaxed);
                    } else {
                        rejected_count.fetch_add(1, Ordering::Relaxed);
                    }
                } else {
                    rejected_count.fetch_add(1, Ordering::Relaxed);
                }

                drop(guard);

                if i % 10 == 0 {
                    tokio::task::yield_now().await;
                }
            }
        }));
    }

    for handle in handles {
        handle.await.unwrap();
    }

    let elapsed = start.elapsed();
    let guard = table.lock().await;
    let stats = guard.stats(Timestamp::new(2000));

    println!(
        "100-attacker flood: {} staged, {} verified, {} rejected in {:?}",
        staged.load(Ordering::Relaxed),
        verified.load(Ordering::Relaxed),
        rejected.load(Ordering::Relaxed),
        elapsed
    );
    println!(
        "Final state: {} peers, {} pending",
        stats.total_peers, stats.pending_verification_count
    );

    // Table should be bounded
    assert!(
        stats.total_peers <= 256 * 20, // 256 buckets * k
        "Routing table overflow: {} peers",
        stats.total_peers
    );

    println!("✅ Peer discovery survived 100-attacker flood");
}

/// BRUTAL TEST: Rapid connect/disconnect churn
#[tokio::test(flavor = "multi_thread", worker_threads = 8)]
async fn brutal_peer_churn_attack() {
    let local_id = NodeId::new([0u8; 32]);
    let config = KademliaConfig::default();
    let table = Arc::new(Mutex::new(RoutingTable::new(local_id, config)));

    let connects = Arc::new(AtomicUsize::new(0));
    let disconnects = Arc::new(AtomicUsize::new(0));

    let mut handles = vec![];

    // 10 churning attackers
    for attacker in 0..10u8 {
        let table_clone = table.clone();
        let conn = connects.clone();
        let disc = disconnects.clone();

        handles.push(tokio::spawn(async move {
            for cycle in 0..100u8 {
                let mut id_bytes = [0u8; 32];
                id_bytes[0] = attacker;
                id_bytes[1] = cycle;
                let node_id = NodeId::new(id_bytes);

                let peer = PeerInfo::new(
                    node_id,
                    SocketAddr::new(IpAddr::v4(10, attacker, cycle, 1), 30303),
                    Timestamp::new(cycle as u64 * 100),
                );

                // Connect
                {
                    let mut guard = table_clone.lock().await;
                    if guard
                        .stage_peer(peer, Timestamp::new(cycle as u64 * 100))
                        .is_ok()
                    {
                        let _ = guard.on_verification_result(
                            &node_id,
                            true,
                            Timestamp::new(cycle as u64 * 100 + 1),
                        );
                        conn.fetch_add(1, Ordering::Relaxed);
                    }
                }

                tokio::task::yield_now().await;

                // Disconnect (mark as failed)
                {
                    let mut guard = table_clone.lock().await;
                    let _ = guard.on_verification_result(
                        &node_id,
                        false,
                        Timestamp::new(cycle as u64 * 100 + 50),
                    );
                    disc.fetch_add(1, Ordering::Relaxed);
                }
            }
        }));
    }

    for handle in handles {
        handle.await.unwrap();
    }

    let guard = table.lock().await;
    let stats = guard.stats(Timestamp::new(20000));

    println!(
        "Churn attack: {} connects, {} disconnects, {} final peers",
        connects.load(Ordering::Relaxed),
        disconnects.load(Ordering::Relaxed),
        stats.total_peers
    );

    // After massive churn, table should still be consistent
    assert!(stats.total_peers <= 256 * 20, "Table overflow after churn");

    println!("✅ Survived peer churn attack");
}
