use super::helpers::{create_tx, MockMempoolGateway};
use qc_06_mempool::domain::{
    entities::MempoolConfig,
    pool::TransactionPool,
};
use std::sync::atomic::{AtomicUsize, Ordering};
use std::sync::Arc;
use std::time::Instant;
use tokio::sync::{Mutex, RwLock};

/// BRUTAL TEST: 50 concurrent attackers, 100 transactions each
///
/// This tests lock contention and race conditions
#[tokio::test(flavor = "multi_thread", worker_threads = 16)]
async fn brutal_mempool_50_thread_hammer() {
    let config = MempoolConfig {
        max_transactions: 1000,
        max_per_account: 50,
        ..MempoolConfig::default()
    };
    let pool = Arc::new(Mutex::new(TransactionPool::new(config)));

    let successful = Arc::new(AtomicUsize::new(0));
    let rejected = Arc::new(AtomicUsize::new(0));
    let errors = Arc::new(AtomicUsize::new(0));

    let start = Instant::now();
    let mut handles = vec![];

    for attacker in 0..50u8 {
        let pool_clone = pool.clone();
        let success = successful.clone();
        let reject = rejected.clone();
        let err = errors.clone();

        handles.push(tokio::spawn(async move {
            for nonce in 0..100u64 {
                let tx = create_tx(attacker, nonce, 1_000_000_000, 1000 + nonce);

                // Minimize lock hold time
                let result = {
                    let mut guard = pool_clone.lock().await;
                    guard.add(tx)
                };

                match result {
                    Ok(_) => {
                        success.fetch_add(1, Ordering::Relaxed);
                    }
                    Err(_) => {
                        reject.fetch_add(1, Ordering::Relaxed);
                    }
                }

                // Yield to create interleaving
                if nonce % 10 == 0 {
                    tokio::task::yield_now().await;
                }
            }
        }));
    }

    for handle in handles {
        if handle.await.is_err() {
            errors.fetch_add(1, Ordering::Relaxed);
        }
    }

    let elapsed = start.elapsed();
    let success_count = successful.load(Ordering::Relaxed);
    let reject_count = rejected.load(Ordering::Relaxed);
    let error_count = errors.load(Ordering::Relaxed);

    let guard = pool.lock().await;
    let final_count = guard.len();

    println!(
        "50-thread hammer: {} success, {} rejected, {} errors in {:?}",
        success_count, reject_count, error_count, elapsed
    );
    println!("Final pool size: {} (max: 1000)", final_count);

    // Invariants
    assert_eq!(error_count, 0, "Task panics detected!");
    assert!(final_count <= 1000, "Pool exceeded max size!");
    assert!(
        success_count + reject_count == 5000,
        "Lost transactions! {} + {} != 5000",
        success_count,
        reject_count
    );

    println!("✅ Mempool survived 50-thread hammer");
}

/// BRUTAL TEST: Read-write contention
///
/// Writers adding, readers querying simultaneously
#[tokio::test(flavor = "multi_thread", worker_threads = 16)]
async fn brutal_mempool_read_write_contention() {
    let config = MempoolConfig {
        max_transactions: 500,
        ..MempoolConfig::default()
    };
    let pool = Arc::new(RwLock::new(TransactionPool::new(config)));

    let writes = Arc::new(AtomicUsize::new(0));
    let reads = Arc::new(AtomicUsize::new(0));
    let inconsistencies = Arc::new(AtomicUsize::new(0));

    let start = Instant::now();
    let mut handles = vec![];

    // 20 writers
    for writer in 0..20u8 {
        let pool_clone = pool.clone();
        let write_count = writes.clone();

        handles.push(tokio::spawn(async move {
            for nonce in 0..50u64 {
                let tx = create_tx(writer, nonce, 1_000_000_000, 1000);
                let mut guard = pool_clone.write().await;
                let _ = guard.add(tx);
                write_count.fetch_add(1, Ordering::Relaxed);
                drop(guard);
                tokio::task::yield_now().await;
            }
        }));
    }

    // 30 readers
    for _ in 0..30 {
        let pool_clone = pool.clone();
        let read_count = reads.clone();
        let inconsistent = inconsistencies.clone();

        handles.push(tokio::spawn(async move {
            for _ in 0..100 {
                let guard = pool_clone.read().await;
                let len = guard.len();
                let status = guard.status(10000);

                // Check consistency: len should match status
                if len != status.pending_count + status.pending_inclusion_count {
                    inconsistent.fetch_add(1, Ordering::Relaxed);
                }

                read_count.fetch_add(1, Ordering::Relaxed);
                drop(guard);
                tokio::task::yield_now().await;
            }
        }));
    }

    for handle in handles {
        handle.await.unwrap();
    }

    let elapsed = start.elapsed();
    let write_count = writes.load(Ordering::Relaxed);
    let read_count = reads.load(Ordering::Relaxed);
    let inconsistent_count = inconsistencies.load(Ordering::Relaxed);

    println!(
        "Read-write contention: {} writes, {} reads, {} inconsistencies in {:?}",
        write_count, read_count, inconsistent_count, elapsed
    );

    assert_eq!(
        inconsistent_count, 0,
        "VULNERABILITY: {} inconsistent reads detected!",
        inconsistent_count
    );

    println!("✅ No read-write inconsistencies");
}
