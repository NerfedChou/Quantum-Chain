//! # Under Pressure: Concurrent Attack Tests
//!
//! The system is under attack from multiple directions simultaneously.
//! No "polite" sequential testing - this is war.
//!
//! ## Attack Scenarios:
//! 1. 500 threads hammering mempool simultaneously
//! 2. Peer discovery table under sustained flood
//! 3. Signature verification under CPU exhaustion
//! 4. Combined multi-vector attack

use qc_01_peer_discovery::{
    IpAddr, KademliaConfig, NodeId, PeerInfo, RoutingTable, SocketAddr, Timestamp,
};
use qc_06_mempool::domain::{
    entities::{MempoolConfig, MempoolTransaction, SignedTransaction, U256},
    pool::TransactionPool,
};
use qc_10_signature_verification::{
    domain::entities::EcdsaSignature, ports::inbound::SignatureVerificationApi,
    service::SignatureVerificationService,
};
use std::sync::atomic::{AtomicU64, AtomicUsize, Ordering};
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::{Mutex, RwLock};

#[derive(Clone)]
struct MockMempoolGateway;

#[async_trait::async_trait]
impl qc_10_signature_verification::ports::outbound::MempoolGateway for MockMempoolGateway {
    async fn submit_verified_transaction(
        &self,
        _tx: qc_10_signature_verification::domain::entities::VerifiedTransaction,
    ) -> Result<(), qc_10_signature_verification::ports::outbound::MempoolError> {
        Ok(())
    }
}

fn create_tx(sender_byte: u8, nonce: u64, gas_price: u64, timestamp: u64) -> MempoolTransaction {
    let signed_tx = SignedTransaction {
        from: [sender_byte; 20],
        to: Some([0xBB; 20]),
        value: U256::zero(),
        nonce,
        gas_price: U256::from(gas_price),
        gas_limit: 21000,
        data: vec![],
        signature: [0u8; 64],
    };
    MempoolTransaction::new(signed_tx, timestamp)
}

// ============================================================================
// MEMPOOL HAMMER TESTS
// ============================================================================

/// BRUTAL TEST: 50 concurrent attackers, 100 transactions each
///
/// This tests lock contention and race conditions
#[tokio::test(flavor = "multi_thread", worker_threads = 16)]
async fn brutal_mempool_50_thread_hammer() {
    let config = MempoolConfig {
        max_transactions: 1000,
        max_per_account: 50,
        ..MempoolConfig::default()
    };
    let pool = Arc::new(Mutex::new(TransactionPool::new(config)));

    let successful = Arc::new(AtomicUsize::new(0));
    let rejected = Arc::new(AtomicUsize::new(0));
    let errors = Arc::new(AtomicUsize::new(0));

    let start = Instant::now();
    let mut handles = vec![];

    for attacker in 0..50u8 {
        let pool_clone = pool.clone();
        let success = successful.clone();
        let reject = rejected.clone();
        let err = errors.clone();

        handles.push(tokio::spawn(async move {
            for nonce in 0..100u64 {
                let tx = create_tx(attacker, nonce, 1_000_000_000, 1000 + nonce);

                // Minimize lock hold time
                let result = {
                    let mut guard = pool_clone.lock().await;
                    guard.add(tx)
                };

                match result {
                    Ok(_) => {
                        success.fetch_add(1, Ordering::Relaxed);
                    }
                    Err(_) => {
                        reject.fetch_add(1, Ordering::Relaxed);
                    }
                }

                // Yield to create interleaving
                if nonce % 10 == 0 {
                    tokio::task::yield_now().await;
                }
            }
        }));
    }

    for handle in handles {
        if handle.await.is_err() {
            errors.fetch_add(1, Ordering::Relaxed);
        }
    }

    let elapsed = start.elapsed();
    let success_count = successful.load(Ordering::Relaxed);
    let reject_count = rejected.load(Ordering::Relaxed);
    let error_count = errors.load(Ordering::Relaxed);

    let guard = pool.lock().await;
    let final_count = guard.len();

    println!(
        "50-thread hammer: {} success, {} rejected, {} errors in {:?}",
        success_count, reject_count, error_count, elapsed
    );
    println!("Final pool size: {} (max: 1000)", final_count);

    // Invariants
    assert_eq!(error_count, 0, "Task panics detected!");
    assert!(final_count <= 1000, "Pool exceeded max size!");
    assert!(
        success_count + reject_count == 5000,
        "Lost transactions! {} + {} != 5000",
        success_count,
        reject_count
    );

    println!("✅ Mempool survived 50-thread hammer");
}

/// BRUTAL TEST: Read-write contention
///
/// Writers adding, readers querying simultaneously
#[tokio::test(flavor = "multi_thread", worker_threads = 16)]
async fn brutal_mempool_read_write_contention() {
    let config = MempoolConfig {
        max_transactions: 500,
        ..MempoolConfig::default()
    };
    let pool = Arc::new(RwLock::new(TransactionPool::new(config)));

    let writes = Arc::new(AtomicUsize::new(0));
    let reads = Arc::new(AtomicUsize::new(0));
    let inconsistencies = Arc::new(AtomicUsize::new(0));

    let start = Instant::now();
    let mut handles = vec![];

    // 20 writers
    for writer in 0..20u8 {
        let pool_clone = pool.clone();
        let write_count = writes.clone();

        handles.push(tokio::spawn(async move {
            for nonce in 0..50u64 {
                let tx = create_tx(writer, nonce, 1_000_000_000, 1000);
                let mut guard = pool_clone.write().await;
                let _ = guard.add(tx);
                write_count.fetch_add(1, Ordering::Relaxed);
                drop(guard);
                tokio::task::yield_now().await;
            }
        }));
    }

    // 30 readers
    for _ in 0..30 {
        let pool_clone = pool.clone();
        let read_count = reads.clone();
        let inconsistent = inconsistencies.clone();

        handles.push(tokio::spawn(async move {
            for _ in 0..100 {
                let guard = pool_clone.read().await;
                let len = guard.len();
                let status = guard.status(10000);

                // Check consistency: len should match status
                if len != status.pending_count + status.pending_inclusion_count {
                    inconsistent.fetch_add(1, Ordering::Relaxed);
                }

                read_count.fetch_add(1, Ordering::Relaxed);
                drop(guard);
                tokio::task::yield_now().await;
            }
        }));
    }

    for handle in handles {
        handle.await.unwrap();
    }

    let elapsed = start.elapsed();
    let write_count = writes.load(Ordering::Relaxed);
    let read_count = reads.load(Ordering::Relaxed);
    let inconsistent_count = inconsistencies.load(Ordering::Relaxed);

    println!(
        "Read-write contention: {} writes, {} reads, {} inconsistencies in {:?}",
        write_count, read_count, inconsistent_count, elapsed
    );

    assert_eq!(
        inconsistent_count, 0,
        "VULNERABILITY: {} inconsistent reads detected!",
        inconsistent_count
    );

    println!("✅ No read-write inconsistencies");
}

// ============================================================================
// PEER DISCOVERY FLOOD TESTS
// ============================================================================

/// BRUTAL TEST: 100 concurrent peer floods
#[tokio::test(flavor = "multi_thread", worker_threads = 16)]
async fn brutal_peer_discovery_100_flood() {
    let local_id = NodeId::new([0u8; 32]);
    let config = KademliaConfig::default();
    let table = Arc::new(Mutex::new(RoutingTable::new(local_id, config)));

    let staged = Arc::new(AtomicUsize::new(0));
    let verified = Arc::new(AtomicUsize::new(0));
    let rejected = Arc::new(AtomicUsize::new(0));

    let start = Instant::now();
    let mut handles = vec![];

    for attacker in 0..100u8 {
        let table_clone = table.clone();
        let staged_count = staged.clone();
        let verified_count = verified.clone();
        let rejected_count = rejected.clone();

        handles.push(tokio::spawn(async move {
            for i in 0..50u8 {
                let mut id_bytes = [0u8; 32];
                id_bytes[0] = attacker;
                id_bytes[1] = i;
                id_bytes[31] = attacker ^ i; // Vary distance

                let node_id = NodeId::new(id_bytes);
                let peer = PeerInfo::new(
                    node_id.clone(),
                    SocketAddr::new(IpAddr::v4(10, attacker, i, 1), 30303),
                    Timestamp::new(1000),
                );

                let mut guard = table_clone.lock().await;

                if guard.stage_peer(peer, Timestamp::new(1000)).is_ok() {
                    staged_count.fetch_add(1, Ordering::Relaxed);

                    if guard
                        .on_verification_result(&node_id, true, Timestamp::new(1001))
                        .is_ok()
                    {
                        verified_count.fetch_add(1, Ordering::Relaxed);
                    } else {
                        rejected_count.fetch_add(1, Ordering::Relaxed);
                    }
                } else {
                    rejected_count.fetch_add(1, Ordering::Relaxed);
                }

                drop(guard);

                if i % 10 == 0 {
                    tokio::task::yield_now().await;
                }
            }
        }));
    }

    for handle in handles {
        handle.await.unwrap();
    }

    let elapsed = start.elapsed();
    let guard = table.lock().await;
    let stats = guard.stats(Timestamp::new(2000));

    println!(
        "100-attacker flood: {} staged, {} verified, {} rejected in {:?}",
        staged.load(Ordering::Relaxed),
        verified.load(Ordering::Relaxed),
        rejected.load(Ordering::Relaxed),
        elapsed
    );
    println!(
        "Final state: {} peers, {} pending",
        stats.total_peers, stats.pending_verification_count
    );

    // Table should be bounded
    assert!(
        stats.total_peers <= 256 * 20, // 256 buckets * k
        "Routing table overflow: {} peers",
        stats.total_peers
    );

    println!("✅ Peer discovery survived 100-attacker flood");
}

/// BRUTAL TEST: Rapid connect/disconnect churn
#[tokio::test(flavor = "multi_thread", worker_threads = 8)]
async fn brutal_peer_churn_attack() {
    let local_id = NodeId::new([0u8; 32]);
    let config = KademliaConfig::default();
    let table = Arc::new(Mutex::new(RoutingTable::new(local_id, config)));

    let connects = Arc::new(AtomicUsize::new(0));
    let disconnects = Arc::new(AtomicUsize::new(0));

    let mut handles = vec![];

    // 10 churning attackers
    for attacker in 0..10u8 {
        let table_clone = table.clone();
        let conn = connects.clone();
        let disc = disconnects.clone();

        handles.push(tokio::spawn(async move {
            for cycle in 0..100u8 {
                let mut id_bytes = [0u8; 32];
                id_bytes[0] = attacker;
                id_bytes[1] = cycle;
                let node_id = NodeId::new(id_bytes);

                let peer = PeerInfo::new(
                    node_id.clone(),
                    SocketAddr::new(IpAddr::v4(10, attacker, cycle, 1), 30303),
                    Timestamp::new(cycle as u64 * 100),
                );

                // Connect
                {
                    let mut guard = table_clone.lock().await;
                    if guard
                        .stage_peer(peer, Timestamp::new(cycle as u64 * 100))
                        .is_ok()
                    {
                        let _ = guard.on_verification_result(
                            &node_id,
                            true,
                            Timestamp::new(cycle as u64 * 100 + 1),
                        );
                        conn.fetch_add(1, Ordering::Relaxed);
                    }
                }

                tokio::task::yield_now().await;

                // Disconnect (mark as failed)
                {
                    let mut guard = table_clone.lock().await;
                    let _ = guard.on_verification_result(
                        &node_id,
                        false,
                        Timestamp::new(cycle as u64 * 100 + 50),
                    );
                    disc.fetch_add(1, Ordering::Relaxed);
                }
            }
        }));
    }

    for handle in handles {
        handle.await.unwrap();
    }

    let guard = table.lock().await;
    let stats = guard.stats(Timestamp::new(20000));

    println!(
        "Churn attack: {} connects, {} disconnects, {} final peers",
        connects.load(Ordering::Relaxed),
        disconnects.load(Ordering::Relaxed),
        stats.total_peers
    );

    // After massive churn, table should still be consistent
    assert!(stats.total_peers <= 256 * 20, "Table overflow after churn");

    println!("✅ Survived peer churn attack");
}

// ============================================================================
// SIGNATURE VERIFICATION EXHAUSTION
// ============================================================================

/// BRUTAL TEST: CPU exhaustion via signature verification
#[tokio::test(flavor = "multi_thread", worker_threads = 16)]
async fn brutal_signature_cpu_exhaustion() {
    let service = Arc::new(SignatureVerificationService::new(MockMempoolGateway));

    let verified = Arc::new(AtomicUsize::new(0));
    let total_time_ns = Arc::new(AtomicU64::new(0));

    let start = Instant::now();
    let mut handles = vec![];

    // 50 threads, each verifying 100 signatures
    for thread in 0..50u8 {
        let svc = service.clone();
        let ver = verified.clone();
        let time = total_time_ns.clone();

        handles.push(tokio::spawn(async move {
            for i in 0..100u8 {
                let mut hash = [0u8; 32];
                hash[0] = thread;
                hash[1] = i;

                let sig = EcdsaSignature {
                    r: [thread; 32],
                    s: [i; 32],
                    v: 27,
                };

                let op_start = Instant::now();
                let _ = svc.verify_ecdsa(&hash, &sig);
                let op_time = op_start.elapsed().as_nanos() as u64;

                time.fetch_add(op_time, Ordering::Relaxed);
                ver.fetch_add(1, Ordering::Relaxed);

                // Don't yield - we want CPU pressure
            }
        }));
    }

    for handle in handles {
        handle.await.unwrap();
    }

    let elapsed = start.elapsed();
    let total_verified = verified.load(Ordering::Relaxed);
    let total_ns = total_time_ns.load(Ordering::Relaxed);
    let avg_ns = total_ns / total_verified as u64;

    println!(
        "CPU exhaustion: {} verifications in {:?}",
        total_verified, elapsed
    );
    println!(
        "Average: {} ns/verification, Throughput: {:.0} verif/sec",
        avg_ns,
        total_verified as f64 / elapsed.as_secs_f64()
    );

    // Should complete all verifications
    assert_eq!(total_verified, 5000, "Lost verifications under load");

    // Should maintain reasonable throughput (at least 1000/sec)
    let throughput = total_verified as f64 / elapsed.as_secs_f64();
    assert!(
        throughput > 1000.0,
        "Throughput too low under load: {:.0} verif/sec",
        throughput
    );

    println!("✅ Signature verification survived CPU exhaustion");
}

// ============================================================================
// MULTI-VECTOR COMBINED ATTACK
// ============================================================================

/// BRUTAL TEST: All three subsystems under simultaneous attack
#[tokio::test(flavor = "multi_thread", worker_threads = 16)]
async fn brutal_multi_vector_attack() {
    // Shared resources
    let mempool = Arc::new(Mutex::new(TransactionPool::new(MempoolConfig {
        max_transactions: 500,
        ..MempoolConfig::default()
    })));

    let routing = Arc::new(Mutex::new(RoutingTable::new(
        NodeId::new([0u8; 32]),
        KademliaConfig::default(),
    )));

    let sig_service = Arc::new(SignatureVerificationService::new(MockMempoolGateway));

    // Metrics
    let mempool_ops = Arc::new(AtomicUsize::new(0));
    let routing_ops = Arc::new(AtomicUsize::new(0));
    let sig_ops = Arc::new(AtomicUsize::new(0));
    let failures = Arc::new(AtomicUsize::new(0));

    let start = Instant::now();
    let duration = Duration::from_secs(3);

    let mut handles = vec![];

    // Mempool attackers (10 threads)
    for attacker in 0..10u8 {
        let pool = mempool.clone();
        let ops = mempool_ops.clone();
        let fail = failures.clone();
        let deadline = start + duration;

        handles.push(tokio::spawn(async move {
            let mut nonce = 0u64;
            while Instant::now() < deadline {
                let tx = create_tx(attacker, nonce, 1_000_000_000, 1000);
                {
                    let mut guard = pool.lock().await;
                    let _ = guard.add(tx);
                }
                ops.fetch_add(1, Ordering::Relaxed);
                nonce += 1;

                if nonce % 50 == 0 {
                    tokio::task::yield_now().await;
                }
            }
        }));
    }

    // Routing attackers (10 threads)
    for attacker in 0..10u8 {
        let table = routing.clone();
        let ops = routing_ops.clone();
        let deadline = start + duration;

        handles.push(tokio::spawn(async move {
            let mut i = 0u8;
            while Instant::now() < deadline {
                let mut id = [0u8; 32];
                id[0] = attacker;
                id[1] = i;
                let node_id = NodeId::new(id);

                let peer = PeerInfo::new(
                    node_id.clone(),
                    SocketAddr::new(IpAddr::v4(10, attacker, i, 1), 30303),
                    Timestamp::new(1000),
                );

                {
                    let mut guard = table.lock().await;
                    let _ = guard.stage_peer(peer, Timestamp::new(1000));
                    let _ = guard.on_verification_result(&node_id, true, Timestamp::new(1001));
                }

                ops.fetch_add(1, Ordering::Relaxed);
                i = i.wrapping_add(1);

                if i % 50 == 0 {
                    tokio::task::yield_now().await;
                }
            }
        }));
    }

    // Signature verification attackers (10 threads)
    for attacker in 0..10u8 {
        let svc = sig_service.clone();
        let ops = sig_ops.clone();
        let deadline = start + duration;

        handles.push(tokio::spawn(async move {
            let mut i = 0u8;
            while Instant::now() < deadline {
                let hash = [attacker ^ i; 32];
                let sig = EcdsaSignature {
                    r: [attacker; 32],
                    s: [i; 32],
                    v: 27,
                };

                let _ = svc.verify_ecdsa(&hash, &sig);
                ops.fetch_add(1, Ordering::Relaxed);
                i = i.wrapping_add(1);
            }
        }));
    }

    // Wait for all attacks
    for handle in handles {
        if handle.await.is_err() {
            failures.fetch_add(1, Ordering::Relaxed);
        }
    }

    let elapsed = start.elapsed();
    let mempool_count = mempool_ops.load(Ordering::Relaxed);
    let routing_count = routing_ops.load(Ordering::Relaxed);
    let sig_count = sig_ops.load(Ordering::Relaxed);
    let failure_count = failures.load(Ordering::Relaxed);

    println!("=== MULTI-VECTOR ATTACK RESULTS ===");
    println!("Duration: {:?}", elapsed);
    println!(
        "Mempool ops: {} ({:.0}/sec)",
        mempool_count,
        mempool_count as f64 / elapsed.as_secs_f64()
    );
    println!(
        "Routing ops: {} ({:.0}/sec)",
        routing_count,
        routing_count as f64 / elapsed.as_secs_f64()
    );
    println!(
        "Sig verify ops: {} ({:.0}/sec)",
        sig_count,
        sig_count as f64 / elapsed.as_secs_f64()
    );
    println!("Task failures: {}", failure_count);

    // Verify system integrity
    {
        let guard = mempool.lock().await;
        assert!(guard.len() <= 500, "Mempool overflow");
    }

    {
        let guard = routing.lock().await;
        let stats = guard.stats(Timestamp::new(10000));
        assert!(stats.total_peers <= 256 * 20, "Routing overflow");
    }

    assert_eq!(failure_count, 0, "Task panics during multi-vector attack");

    println!("✅ All subsystems survived multi-vector attack");
}

// ============================================================================
// RESOURCE STARVATION TESTS
// ============================================================================

/// BRUTAL TEST: Lock starvation attack
///
/// One thread holds lock for extended time, others starve
#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
async fn brutal_lock_starvation() {
    let pool = Arc::new(Mutex::new(TransactionPool::new(MempoolConfig::default())));

    let starved_attempts = Arc::new(AtomicUsize::new(0));
    let starved_successes = Arc::new(AtomicUsize::new(0));

    let mut handles = vec![];

    // Hog thread - holds lock for extended periods
    let hog_pool = pool.clone();
    handles.push(tokio::spawn(async move {
        for _ in 0..10 {
            let guard = hog_pool.lock().await;
            // Simulate expensive operation while holding lock
            tokio::time::sleep(Duration::from_millis(50)).await;
            drop(guard);
            tokio::task::yield_now().await;
        }
    }));

    // Victim threads - try to access during hogging
    for victim in 0..5u8 {
        let victim_pool = pool.clone();
        let attempts = starved_attempts.clone();
        let successes = starved_successes.clone();

        handles.push(tokio::spawn(async move {
            for nonce in 0..20u64 {
                attempts.fetch_add(1, Ordering::Relaxed);

                let tx = create_tx(victim, nonce, 1_000_000_000, 1000);
                let result = {
                    let mut guard = victim_pool.lock().await;
                    guard.add(tx)
                };

                if result.is_ok() {
                    successes.fetch_add(1, Ordering::Relaxed);
                }

                tokio::task::yield_now().await;
            }
        }));
    }

    for handle in handles {
        handle.await.unwrap();
    }

    let attempts = starved_attempts.load(Ordering::Relaxed);
    let successes = starved_successes.load(Ordering::Relaxed);

    println!(
        "Lock starvation: {} attempts, {} successes ({:.1}% success rate)",
        attempts,
        successes,
        (successes as f64 / attempts as f64) * 100.0
    );

    // All attempts should eventually succeed (fairness)
    assert!(
        successes > attempts / 2,
        "Too many starved operations: {}/{} succeeded",
        successes,
        attempts
    );

    println!("✅ Lock fairness maintained under starvation attack");
}
