//! # Breach Isolation: Container Isolation Tests
//!
//! If one subsystem is compromised, do others survive?
//! Simulates complete subsystem failures and resource exhaustion.
//!
//! ## Attack Scenarios:
//! 1. Memory exhaustion in one subsystem
//! 2. Panic in one subsystem's handler
//! 3. Deadlock in one subsystem
//! 4. Resource leak detection

use qc_01_peer_discovery::{
    IpAddr, KademliaConfig, NodeId, PeerInfo, RoutingTable, SocketAddr, Timestamp,
};
use qc_06_mempool::domain::{
    entities::{MempoolConfig, MempoolTransaction, SignedTransaction, U256},
    pool::TransactionPool,
};
use qc_10_signature_verification::{
    domain::entities::EcdsaSignature, ports::inbound::SignatureVerificationApi,
    service::SignatureVerificationService,
};
use std::sync::atomic::{AtomicBool, AtomicUsize, Ordering};
use std::sync::Arc;
use std::time::Duration;
use tokio::sync::Mutex;

#[derive(Clone)]
struct MockMempoolGateway;

#[async_trait::async_trait]
impl qc_10_signature_verification::ports::outbound::MempoolGateway for MockMempoolGateway {
    async fn submit_verified_transaction(
        &self,
        _tx: qc_10_signature_verification::domain::entities::VerifiedTransaction,
    ) -> Result<(), qc_10_signature_verification::ports::outbound::MempoolError> {
        Ok(())
    }
}

fn create_tx(sender_byte: u8, nonce: u64, gas_price: u64) -> MempoolTransaction {
    let signed_tx = SignedTransaction {
        from: [sender_byte; 20],
        to: Some([0xBB; 20]),
        value: U256::zero(),
        nonce,
        gas_price: U256::from(gas_price),
        gas_limit: 21000,
        data: vec![],
        signature: [0u8; 64],
    };
    MempoolTransaction::new(signed_tx, 1000)
}

fn create_large_tx(sender_byte: u8, nonce: u64, data_size: usize) -> MempoolTransaction {
    let signed_tx = SignedTransaction {
        from: [sender_byte; 20],
        to: Some([0xBB; 20]),
        value: U256::zero(),
        nonce,
        gas_price: U256::from(1_000_000_000u64),
        gas_limit: 21000 + (data_size as u64 * 16),
        data: vec![0xDE; data_size],
        signature: [0u8; 64],
    };
    MempoolTransaction::new(signed_tx, 1000)
}

// ============================================================================
// MEMORY ISOLATION TESTS
// ============================================================================

/// BRUTAL TEST: Memory exhaustion in mempool doesn't affect peer discovery
#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
async fn brutal_mempool_exhaustion_peer_isolation() {
    // Mempool with small limit
    let mempool = Arc::new(Mutex::new(TransactionPool::new(MempoolConfig {
        max_transactions: 100,
        ..MempoolConfig::default()
    })));

    // Peer discovery (independent)
    let routing = Arc::new(Mutex::new(RoutingTable::new(
        NodeId::new([0u8; 32]),
        KademliaConfig::default(),
    )));

    let mempool_alive = Arc::new(AtomicBool::new(true));
    let routing_alive = Arc::new(AtomicBool::new(true));

    let mut handles = vec![];

    // Exhaust mempool
    let mp = mempool.clone();
    let mp_alive = mempool_alive.clone();
    handles.push(tokio::spawn(async move {
        // Fill with large transactions to exhaust memory
        for i in 0..200u8 {
            let tx = create_large_tx(i, 0, 10 * 1024); // 10KB each
            let mut guard = mp.lock().await;
            match guard.add(tx) {
                Ok(_) => {}
                Err(_) => {
                    // Pool full - this is expected
                }
            }
        }
        mp_alive.store(true, Ordering::SeqCst);
    }));

    // Concurrently use peer discovery
    let rt = routing.clone();
    let rt_alive = routing_alive.clone();
    handles.push(tokio::spawn(async move {
        for i in 0..50u8 {
            let node_id = NodeId::new([i; 32]);
            let peer = PeerInfo::new(
                node_id.clone(),
                SocketAddr::new(IpAddr::v4(10, 0, 0, i), 30303),
                Timestamp::new(1000),
            );

            let mut guard = rt.lock().await;
            let _ = guard.stage_peer(peer, Timestamp::new(1000));
            let _ = guard.on_verification_result(&node_id, true, Timestamp::new(1001));
        }
        rt_alive.store(true, Ordering::SeqCst);
    }));

    // Wait for both
    for handle in handles {
        handle.await.unwrap();
    }

    // Both should be alive
    assert!(mempool_alive.load(Ordering::SeqCst), "Mempool task died");
    assert!(
        routing_alive.load(Ordering::SeqCst),
        "Routing task died during mempool exhaustion"
    );

    // Verify routing still works
    {
        let guard = routing.lock().await;
        let stats = guard.stats(Timestamp::new(2000));
        assert!(stats.total_peers > 0, "Routing table should have peers");
    }

    println!("✅ Peer discovery survived mempool exhaustion");
}

/// BRUTAL TEST: Peer discovery stress doesn't affect mempool
#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
async fn brutal_peer_stress_mempool_isolation() {
    let mempool = Arc::new(Mutex::new(TransactionPool::new(MempoolConfig::default())));
    let routing = Arc::new(Mutex::new(RoutingTable::new(
        NodeId::new([0u8; 32]),
        KademliaConfig::default(),
    )));

    let mempool_ops = Arc::new(AtomicUsize::new(0));
    let routing_ops = Arc::new(AtomicUsize::new(0));

    let mut handles = vec![];

    // Stress peer discovery
    for attacker in 0..20u8 {
        let rt = routing.clone();
        let ops = routing_ops.clone();

        handles.push(tokio::spawn(async move {
            for i in 0..100u8 {
                let mut id = [0u8; 32];
                id[0] = attacker;
                id[1] = i;
                let node_id = NodeId::new(id);

                let peer = PeerInfo::new(
                    node_id.clone(),
                    SocketAddr::new(IpAddr::v4(10, attacker, i, 1), 30303),
                    Timestamp::new(1000),
                );

                let mut guard = rt.lock().await;
                let _ = guard.stage_peer(peer, Timestamp::new(1000));
                ops.fetch_add(1, Ordering::Relaxed);
            }
        }));
    }

    // Concurrently use mempool (should be unaffected)
    for user in 0..10u8 {
        let mp = mempool.clone();
        let ops = mempool_ops.clone();

        handles.push(tokio::spawn(async move {
            for nonce in 0..20u64 {
                let tx = create_tx(user, nonce, 1_000_000_000);
                let mut guard = mp.lock().await;
                if guard.add(tx).is_ok() {
                    ops.fetch_add(1, Ordering::Relaxed);
                }
            }
        }));
    }

    for handle in handles {
        handle.await.unwrap();
    }

    let mempool_count = mempool_ops.load(Ordering::Relaxed);
    let routing_count = routing_ops.load(Ordering::Relaxed);

    println!(
        "Isolation test: mempool {} ops, routing {} ops",
        mempool_count, routing_count
    );

    // Mempool should have processed normally
    assert!(mempool_count > 100, "Mempool was blocked by routing stress");

    println!("✅ Mempool unaffected by peer discovery stress");
}

// ============================================================================
// PANIC RECOVERY TESTS
// ============================================================================

/// BRUTAL TEST: Subsystem "crash" recovery
///
/// Simulates a subsystem panicking and verifies others continue
#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
async fn brutal_subsystem_crash_isolation() {
    let mempool = Arc::new(Mutex::new(TransactionPool::new(MempoolConfig::default())));
    let routing = Arc::new(Mutex::new(RoutingTable::new(
        NodeId::new([0u8; 32]),
        KademliaConfig::default(),
    )));

    let crash_detected = Arc::new(AtomicBool::new(false));
    let survivor_alive = Arc::new(AtomicBool::new(false));

    // Task that will "crash" (but we catch it)
    let crash = crash_detected.clone();
    let crash_handle = tokio::spawn(async move {
        // Simulate some work
        tokio::time::sleep(Duration::from_millis(50)).await;

        // "Crash" - return error instead of panic
        crash.store(true, Ordering::SeqCst);
        Err::<(), &str>("Simulated crash")
    });

    // Survivor task that should continue
    let survivor = survivor_alive.clone();
    let mp = mempool.clone();
    let rt = routing.clone();
    let survivor_handle = tokio::spawn(async move {
        // Keep working while "crash" happens
        for i in 0..100 {
            // Mempool work
            {
                let tx = create_tx(i as u8, 0, 1_000_000_000);
                let mut guard = mp.lock().await;
                let _ = guard.add(tx);
            }

            // Routing work
            {
                let node_id = NodeId::new([i as u8; 32]);
                let peer = PeerInfo::new(
                    node_id.clone(),
                    SocketAddr::new(IpAddr::v4(10, 0, 0, i as u8), 30303),
                    Timestamp::new(1000),
                );
                let mut guard = rt.lock().await;
                let _ = guard.stage_peer(peer, Timestamp::new(1000));
            }

            tokio::time::sleep(Duration::from_millis(1)).await;
        }

        survivor.store(true, Ordering::SeqCst);
        Ok::<(), &str>(())
    });

    // Wait for crash
    let crash_result = crash_handle.await.unwrap();
    assert!(crash_result.is_err(), "Crash should have occurred");

    // Wait for survivor
    let survivor_result = survivor_handle.await.unwrap();
    assert!(survivor_result.is_ok(), "Survivor should succeed");

    assert!(
        crash_detected.load(Ordering::SeqCst),
        "Crash should have been detected"
    );
    assert!(
        survivor_alive.load(Ordering::SeqCst),
        "Survivor should have completed"
    );

    // Verify state is intact
    {
        let guard = mempool.lock().await;
        assert!(guard.len() > 0, "Mempool should have transactions");
    }
    {
        let guard = routing.lock().await;
        let stats = guard.stats(Timestamp::new(2000));
        assert!(
            stats.total_peers > 0 || stats.pending_verification_count > 0,
            "Routing should have peers"
        );
    }

    println!("✅ Survivor subsystems continued after crash");
}

// ============================================================================
// RESOURCE LEAK DETECTION
// ============================================================================

/// BRUTAL TEST: Detect resource leaks under repeated create/destroy
#[tokio::test]
async fn brutal_resource_leak_detection() {
    let iterations = 100;
    let mut memory_samples = Vec::with_capacity(iterations);

    for i in 0..iterations {
        // Create subsystems
        let mut mempool = TransactionPool::new(MempoolConfig {
            max_transactions: 100,
            ..MempoolConfig::default()
        });

        let mut routing = RoutingTable::new(NodeId::new([0u8; 32]), KademliaConfig::default());

        // Fill them up
        for j in 0..50u8 {
            let tx = create_tx(j, 0, 1_000_000_000);
            let _ = mempool.add(tx);

            let node_id = NodeId::new([j; 32]);
            let peer = PeerInfo::new(
                node_id.clone(),
                SocketAddr::new(IpAddr::v4(10, 0, 0, j), 30303),
                Timestamp::new(1000),
            );
            let _ = routing.stage_peer(peer, Timestamp::new(1000));
        }

        // Get memory snapshot (approximate via status)
        let status = mempool.status(10000);
        memory_samples.push(status.memory_bytes);

        // Drop happens here
    }

    // Check for memory growth trend
    let first_quarter: usize = memory_samples[..25].iter().sum::<usize>() / 25;
    let last_quarter: usize = memory_samples[75..].iter().sum::<usize>() / 25;

    // Memory shouldn't grow more than 20% over time
    let growth = last_quarter as f64 / first_quarter as f64;

    println!(
        "Memory trend: first quarter avg = {}, last quarter avg = {}, growth = {:.2}x",
        first_quarter, last_quarter, growth
    );

    assert!(
        growth < 1.2,
        "VULNERABILITY: Memory leak detected! Growth: {:.2}x",
        growth
    );

    println!("✅ No memory leaks detected");
}

/// BRUTAL TEST: Verify cleanup on full pool
#[test]
fn brutal_cleanup_on_full_pool() {
    let config = MempoolConfig {
        max_transactions: 50,
        pending_inclusion_timeout_ms: 100,
        ..MempoolConfig::default()
    };
    let mut pool = TransactionPool::new(config);

    // Fill pool
    let mut hashes = Vec::new();
    for i in 0..50u8 {
        let tx = create_tx(i, 0, 1_000_000_000);
        hashes.push(tx.hash);
        pool.add(tx).unwrap();
    }

    assert_eq!(pool.len(), 50, "Pool should be full");

    // Move half to pending_inclusion
    let first_half: Vec<_> = hashes.iter().take(25).cloned().collect();
    pool.propose(&first_half, 1, 0);

    assert_eq!(pool.pending_inclusion_count(), 25);
    assert_eq!(pool.pending_count(), 25);

    // Wait for timeout
    std::thread::sleep(std::time::Duration::from_millis(150));

    // Cleanup
    let recovered = pool.cleanup_timeouts(200);

    println!(
        "Cleanup: {} recovered, {} pending, {} pending_inclusion",
        recovered.len(),
        pool.pending_count(),
        pool.pending_inclusion_count()
    );

    // All should be back to pending
    assert_eq!(pool.pending_inclusion_count(), 0, "All should be recovered");
    assert_eq!(pool.pending_count(), 50, "All should be pending again");

    println!("✅ Cleanup correctly recovered timed-out transactions");
}

// ============================================================================
// STATE CONSISTENCY TESTS
// ============================================================================

/// BRUTAL TEST: State consistency after partial operations
#[tokio::test]
async fn brutal_state_consistency() {
    let pool = Arc::new(Mutex::new(TransactionPool::new(MempoolConfig {
        max_transactions: 100,
        ..MempoolConfig::default()
    })));

    let inconsistencies = Arc::new(AtomicUsize::new(0));

    let mut handles = vec![];

    // Multiple threads doing add/remove/query
    for thread in 0..10 {
        let p = pool.clone();
        let inc = inconsistencies.clone();

        handles.push(tokio::spawn(async move {
            for i in 0..50u8 {
                let tx = create_tx(thread * 10 + i, 0, 1_000_000_000);
                let hash = tx.hash;

                // Add
                {
                    let mut guard = p.lock().await;
                    let _ = guard.add(tx);
                }

                // Verify consistency
                {
                    let guard = p.lock().await;
                    let len = guard.len();
                    let status = guard.status(10000);

                    let expected = status.pending_count + status.pending_inclusion_count;
                    if len != expected {
                        inc.fetch_add(1, Ordering::Relaxed);
                    }

                    // If hash exists, it should be gettable
                    if guard.contains(&hash) {
                        if guard.get(&hash).is_none() {
                            inc.fetch_add(1, Ordering::Relaxed);
                        }
                    }
                }

                tokio::task::yield_now().await;
            }
        }));
    }

    for handle in handles {
        handle.await.unwrap();
    }

    let count = inconsistencies.load(Ordering::Relaxed);

    assert_eq!(
        count, 0,
        "VULNERABILITY: {} state inconsistencies detected!",
        count
    );

    println!("✅ State consistency maintained under concurrent operations");
}

/// BRUTAL TEST: Cross-subsystem state isolation
#[tokio::test]
async fn brutal_cross_subsystem_state_isolation() {
    // Create two independent instances of each subsystem
    let mempool1 = Arc::new(Mutex::new(TransactionPool::new(MempoolConfig::default())));
    let mempool2 = Arc::new(Mutex::new(TransactionPool::new(MempoolConfig::default())));

    let routing1 = Arc::new(Mutex::new(RoutingTable::new(
        NodeId::new([0x11; 32]),
        KademliaConfig::default(),
    )));
    let routing2 = Arc::new(Mutex::new(RoutingTable::new(
        NodeId::new([0x22; 32]),
        KademliaConfig::default(),
    )));

    // Fill mempool1 only
    {
        let mut guard = mempool1.lock().await;
        for i in 0..100u8 {
            let tx = create_tx(i, 0, 1_000_000_000);
            guard.add(tx).unwrap();
        }
    }

    // Fill routing1 only
    {
        let mut guard = routing1.lock().await;
        for i in 0..50u8 {
            let node_id = NodeId::new([i; 32]);
            let peer = PeerInfo::new(
                node_id.clone(),
                SocketAddr::new(IpAddr::v4(10, 0, 0, i), 30303),
                Timestamp::new(1000),
            );
            let _ = guard.stage_peer(peer, Timestamp::new(1000));
            let _ = guard.on_verification_result(&node_id, true, Timestamp::new(1001));
        }
    }

    // Verify isolation
    {
        let guard1 = mempool1.lock().await;
        let guard2 = mempool2.lock().await;

        assert_eq!(guard1.len(), 100, "Mempool1 should have 100 txs");
        assert_eq!(guard2.len(), 0, "Mempool2 should be empty");
    }

    {
        let guard1 = routing1.lock().await;
        let guard2 = routing2.lock().await;

        let stats1 = guard1.stats(Timestamp::new(2000));
        let stats2 = guard2.stats(Timestamp::new(2000));

        assert!(stats1.total_peers > 0, "Routing1 should have peers");
        assert_eq!(stats2.total_peers, 0, "Routing2 should be empty");
    }

    println!("✅ Cross-subsystem state isolation verified");
}
